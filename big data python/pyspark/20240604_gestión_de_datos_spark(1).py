# -*- coding: utf-8 -*-
"""20240604 Gestión de Datos Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_XULOnTTB-IdVWUCIYc3UcQyzir9YklA

**Gestión de datos con Spark**
"""

# Java, Apache Spark
!python --version

!apt install neofetch

!neofetch

# Instalar el SDK de Java (opnejdk)
!apt-get install  openjdk-21-jdk-headless -qq > /dev/null

!java --version

# Descargar Apache Spark
!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz

# Descomprimir ApacheSpark
!tar -xvf spark-3.5.1-bin-hadoop3.tgz

# Instalar la libreria findspark
!pip install -q findspark
# Instalar pyspark
!pip install pyspark

# configurar las variables de entorno
import os
# java
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-21-openjdk-amd64"
# Spark
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

# Verificar la instalación
import findspark
findspark.init()
from pyspark.sql import SparkSession

# Crear una sesión
spark = SparkSession.builder.getOrCreate()
spark

df = spark.createDataFrame([{"Saludar": "Wake up, pay attention"}
                            for x in range(10)])
df.show()

# Personalizar la sesión de spark
spark = SparkSession.builder.master("local[*]").appName("MiApp").getOrCreate()
sc  = spark.sparkContext

"""# RDD (Resilient Distributed Dataset)"""

## Crear rdd
# RDD Vacio
rdd_vacio = sc.emptyRDD

# RDD con parallelize
rdd_vacio2 = sc.parallelize([], 3) # datos, particiones

# cuando creo un RDD se asignan particiones
rdd_vacio2.getNumPartitions()

rdd_vacio.getNumPartitions() # no tiene ninguna particion

rdd_vacio3 = sc.parallelize([])
rdd_vacio3.getNumPartitions()

# RDD a partir de una colección de datos
numbers = [2,5,7,9,10,12,14,16,20,23]
rdd = sc.parallelize(numbers)
rdd.collect()

rdd.getNumPartitions()

# RDD a partir de un archivo de texto
rdd_texto = sc.textFile('/content/example_file.txt')
rdd_texto.collect()

# RDD a partir de un archivo de texto (en un sólo registro)
rdd_texto_completo = sc.wholeTextFiles('/content/example_file.txt')
rdd_texto_completo.collect()

# RDD a partir de otro RDD
rdd_suma = rdd.map(lambda x: x + 10)
rdd.collect(), rdd_suma.collect()

# RDD a partir de un dataframe
df = spark.createDataFrame([
    (1,'Sistemas'),
    (2,'Industrial'),
    (3,'Minas'),
    (4, 'Mecatrónica'),
    (5, 'Electrónica')],
    ['ID','Carrera'])
df.show()

rdd_df  = df.rdd
rdd_df.collect()

"""**Transformaciones**

***Map***
"""

rdd = sc.parallelize([2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 23])
rdd_resta = rdd.map(lambda x: x - 5) # restar  5
rdd.collect(), rdd_resta.collect()

rdd_par = rdd.map(lambda x: x%2==0) # Elemento par
rdd.collect(), rdd_par.collect()

rdd_nombres = sc.parallelize([
    'ruben','jurgen','santos','brisa','esmeralda', 'sheyla','isabelle'
])

# crear un rdd que a partir de rdd_nombres, pero que los nombres esten en mayusculas
rdd_mayusc = rdd_nombres.map(lambda x: x.upper())
rdd_mayusc.collect()

# Saludar a cada elemento del rdd_nombres, mostrar el nombre como nombre propio
# Hola Ruben
# Hola Jurgen

"""**Transformaciones: flatMap()**"""

rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
rdd_cuadrado = rdd.map(lambda x: (x, x**2))
rdd_cuadrado.collect()

rdd_cuadrado = rdd.flatMap(lambda x: (x, x**2))
rdd_cuadrado.collect()

rdd_asistentes = sc.parallelize(['alexa','siri','cortana','gemini'])
# map -> ('alexa', 'ALEXA', 'Alexa', 'Hello Alexa'),
#        ('siri', 'SIRI', 'Siri', 'Hello Siri')

# flatMap -> 'alexa', 'ALEXA', 'Alexa', 'Hello Alexa', 'siri', 'SIRI', 'Siri', 'Hello Siri'

"""**Transformaciones: funcion filter()**"""

rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])
# Mostrar solo los pares
rdd_par = rdd.filter(lambda x: x%2==0)
rdd_par.collect()

# mostrar los impares, mostrar los mayores a 9, mostrar los menores a 11
# mostrar los impares mayores que 10 y menores que 15
rdd_m10 = rdd.filter(lambda x: x%2==1 and x > 10 and x < 15)
rdd_m10.collect()

rdd_noms = sc.parallelize(['wills','tania','melisa', 'andres', 'karla', 'karina',
                           'william', 'willy', 'wilson', 'waldir', 'karen',
                           'jimena','ginebra'])
rdd_noms.collect()

# filtrar todos los nombres que empiezan con z
rdd_filterz = rdd_noms.filter(lambda x: x.startswith('w'))
rdd_filterz.collect()

rdd_filterz = rdd_noms.filter(lambda x: x[0] == 'w')
rdd_filterz.collect()

# filtrar los nombres que el 2do caracter sea i y el 4to caracter sea l (ele)
# filtrar los nombres que terminan en a

# filtrar todos los nombres que contengan al menos una s
rdd_n1 = rdd_noms.filter(lambda x: x.find('s') >= 0)
rdd_n1.collect()

rdd_n2 = rdd_noms.filter(lambda x: 's' in x)
rdd_n2.collect()

rdd_n3 = rdd_noms.filter(lambda x: x.__contains__('s'))
rdd_n3.collect()

"""**Transformaciones: coalesce**"""

rdd = sc.parallelize([item ** 2 for item in range(1,11)], 20)
rdd.collect()

print("Número de particiones")
rdd.getNumPartitions()

rdd5 = rdd.coalesce(5)
rdd5.getNumPartitions()

rdd5.collect()